{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1>Problem Statement</h1>\n",
    "\n",
    " <p>Organizing and participating in effective study sessions for the data community is challenging due to differing schedules, academic backgrounds, and geographical locations. Individuals often struggle to find like-minded peers for collaboration, resulting in missed opportunities for learning and knowledge exchange. The MVP aims to address these challenges by providing an Intelligent Session Matching system to connect users based on common availability and interests</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1>Solution</h1>\n",
    "\n",
    " <p>The goal is to develop a **K-Means based model** to group students based on their similarities (availability, skills, location, etc.) and assign them to groups for study sessions. This approach helps streamline the process of matching users for study groups.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dataset\n",
    "\n",
    " The dataset used to train this model has the following fields:\n",
    "\n",
    " 1. **UserID**: Uniquely identifies each user. Not used for K-Means but helps distinguish users.\n",
    "\n",
    " 2. **Latitude**: Represents the user's geographic latitude, used to match users who are close to each other.\n",
    "\n",
    " 3. **Longitude**: Represents the user's geographic longitude, also used for proximity-based matching.\n",
    "\n",
    " 4. **Availability (Hour_0 to Hour_23)**: Binary values (0 or 1) representing the user's hourly availability.\n",
    "\n",
    " 5. **Days_Available (Monday to Sunday)**: Binary values indicating which days of the week the user is available.\n",
    "\n",
    " 6. **Skill_Level**: Indicates the user's skill level (e.g., Beginner, Intermediate, Advanced).\n",
    "\n",
    " 7. **Preferred_Group_Size**: User's preference for group size (Small, Medium, Large).\n",
    "\n",
    " 8. **Topics of Interest**: One-hot encoded columns representing the user's areas of interest (e.g., Python, Machine Learning, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./data/data.csv\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data Preprocessing\n",
    "\n",
    " To prepare the dataset for the K-Means algorithm, we need to:\n",
    "\n",
    " 1. **Convert categorical features**: Categorical data such as `Skill_Level` and `Preferred_Group_Size` should be encoded into numeric values.\n",
    "\n",
    " 2. **Normalize numerical features**: Numerical features with large values (e.g., Latitude and Longitude) should be normalized to avoid bias in the distance calculations used in K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Data Conversion\n",
    "\n",
    " We use the `LabelEncoder` to convert categorical columns like `Skill_Level` and `Preferred_Group_Size` to numerical form so that K-Means can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Convert categorical data into numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data['Skill_Level'] = label_encoder.fit_transform(data['Skill_Level'])\n",
    "data['Preferred_Group_Size'] = label_encoder.fit_transform(data['Preferred_Group_Size'])\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of Numeric Features\n",
    "\n",
    " We use `StandardScaler` to normalize numerical features like `Latitude` and `Longitude` to ensure that the distance metric used by K-Means is not biased toward larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numeric_features = ['Latitude', 'Longitude']\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "\n",
    "# Check the processed data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Unnecessary Columns\n",
    "\n",
    " We drop columns such as `UserID` that are not needed for clustering. K-Means relies on numerical data, and `UserID` is just an identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"UserID\",axis=1)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split\n",
    "\n",
    " We split the dataset into a training set and a test set with 20% of the data being used for testing. This ensures we can validate the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,_,_ = train_test_split(data,[0] * 1000,test_size=0.20,random_state=42)\n",
    "print(X_train.head())\n",
    "print(X_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Optimal `k` Using the Elbow Method\n",
    "\n",
    " The Elbow Method is used to determine the optimal number of clusters (`k`). By plotting the sum of squared errors (SSE) for different values of `k`, we can identify where the SSE starts to level off, indicating the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the Elbow Method to find the optimal number of clusters\n",
    "sse = []  # Sum of squared errors\n",
    "k_range = range(1, 100)  # Test for values of k from 1 to 20\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    sse.append(kmeans.inertia_)  # Inertia is the sum of squared distances to the nearest cluster center\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.plot(k_range, sse, \"bx-\")\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Sum of squared errors (SSE)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Clustering with Silhouette Score\n",
    "\n",
    " The **Silhouette Score** is a metric used to evaluate how well the clustering is performing. A higher silhouette score means better-defined clusters. We can try multiple values of `k` to find the one that maximizes the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "range_n_clusters = range(2, 100)\n",
    "silhouette_avg = []\n",
    "\n",
    "for num_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(data)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    silhouette_avg.append(silhouette_score(data, cluster_labels))\n",
    "\n",
    "plt.plot(range_n_clusters, silhouette_avg, \"bx-\")\n",
    "plt.xlabel(\"Values of K\") \n",
    "plt.ylabel(\"Silhouette score\") \n",
    "plt.title(\"Silhouette analysis For Optimal k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the K-Means Model with Selected `k`\n",
    "\n",
    " Based on the Elbow Method and Silhouette Score, we choose an optimal value for `k` and apply the K-Means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2>Building the KMeans model based on selected k parameter</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal number of clusters\n",
    "k_optimal = 75  # Use the best value from the analysis\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "X_train['Cluster'] = kmeans.fit_predict(X_train)  # Assign cluster labels to each user\n",
    "\n",
    "# Check the cluster assignments\n",
    "print(X_train['Cluster'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Variance Within Each Cluster\n",
    "\n",
    "We calculate the variance within each cluster to ensure that users within the same cluster are similar in terms of their features (e.g., skill level, preferred group size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0\n",
    "for cluster_id in X_train['Cluster'].unique():\n",
    "    cluster_data = X_train[X_train['Cluster'] == cluster_id]\n",
    "    variance = cluster_data[['Skill_Level', 'Preferred_Group_Size', 'Big Data', 'Data Analysis', 'Machine Learning', 'Python', 'SQL', 'Statistics']].var().mean()\n",
    "    assert variance < 1, f\"High variance found in cluster {cluster_id}\"  # Adjust threshold as necessary\n",
    "    avg += variance\n",
    "    print(variance)\n",
    "print(avg / k_optimal)\n",
    "print(\"Within Cluster Variance Test: PASS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Clusters Using PCA\n",
    "\n",
    " To visualize the clustering results, we use **Principal Component Analysis (PCA)** to reduce the dataset to two dimensions and plot the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensionality to 2 components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train.drop(columns=['Cluster'],inplace=False))\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=X_train['Cluster'], cmap='viridis')\n",
    "plt.title('K-Means Clustering of Users')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Clustered Data\n",
    "\n",
    " We save the training dataset with the assigned cluster labels to a CSV file for further analysis or use in the session matching application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./data/output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this project, we developed a K-Means based model to cluster users for organizing study groups based on their availability, skill level, location, and interests. We used the Elbow Method and Silhouette Score to determine the optimal number of clusters (`k`), validated the results by checking the within-cluster variance, and visualized the clusters using PCA.\n",
    "\n",
    "Further improvements can include:\n",
    "1. Testing with real-world data to improve clustering accuracy.\n",
    "2. Refining the model by adding additional features.\n",
    "3. Automating the session-matching process based on the user's nearest cluster.\n",
    "\n",
    "This solution helps in efficiently organizing users into study groups, facilitating collaboration and learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-group",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
